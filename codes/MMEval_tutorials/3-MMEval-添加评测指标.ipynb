{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a64982c",
   "metadata": {},
   "source": [
    "# 1. 为 MMEval 添加一个评测指标 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1b2d9",
   "metadata": {},
   "source": [
    "在 MMEval 中实现一个自定义评测指标，需要继承 BaseMetric 并且实现 add 和 compute_metric 方法。\n",
    "\n",
    "在评测过程中，评测指标需要在调用 add 后更新 _results 以存储中间结果。在最后进行指标计算的时候，将会对 _results 进行进程同步后调用 compute_metric 进行指标的计算。\n",
    "\n",
    "以实现 Accuracy 指标为例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mmeval.core import BaseMetric\n",
    "\n",
    "class Accuracy(BaseMetric):\n",
    "\n",
    "    def add(self, predictions, labels):\n",
    "        for prediction, label in zip(predictions, labels):\n",
    "            self._results.append((prediction, label))\n",
    "\n",
    "    def compute_metric(self, results):\n",
    "        predictions = np.concatenate(\n",
    "            [res[0] for res in results])\n",
    "        labels = np.concatenate(\n",
    "            [res[1] for res in results])\n",
    "        correct = (predictions == labels)\n",
    "        accuracy = sum(correct) / len(predictions)\n",
    "        return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10eadc2",
   "metadata": {},
   "source": [
    "## 2. 从原 OpenMMLab 2.0 算法库中迁移评测指标至 MMEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e4397",
   "metadata": {},
   "source": [
    "虽然 MMEval 目前已经发布了，但是仍然有一大部分原 OpenMMLab 算法库中的评测指标尚未迁移添加到 MMEval 中，我们整理了这些尚未添加的评测指标，发布了社区任务，欢迎各位小伙伴一起来提 PR 参与建设 MMEval：https://github.com/open-mmlab/mmeval/issues/50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6b89b",
   "metadata": {},
   "source": [
    "详细的注意事项可参考：[MMEval 适配指南](https://aicarrier.feishu.cn/docs/doccnjJiEdOYTdse9zH9Y2Bt8De)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73a8ee",
   "metadata": {},
   "source": [
    "下面以 mmseg 中的 [IoUMetric](https://github.com/open-mmlab/mmsegmentation/blob/a06bf4d66349f19d2384bfdb5085d2d0f6ee98e0/mmseg/evaluation/metrics/iou_metric.py#L15) 为例，为大家展示如何将一个 OpenMMLab 2.0 算法库中的评测指标迁移至 MMEval，并在原算法库中适配使用 MMEval。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b81183a",
   "metadata": {},
   "source": [
    "## 2.1 为 MMEval 添加 MeanIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d138e",
   "metadata": {},
   "source": [
    "见：[mmeval/metrics/mean_iou.py](https://github.com/open-mmlab/mmeval/blob/e2f6dbf160d41adfa185caed211f2ac50747f3ba/mmeval/metrics/mean_iou.py#L21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "\n",
    "import numpy as np\n",
    "from typing import TYPE_CHECKING, List, Optional, Sequence, Tuple, overload\n",
    "\n",
    "from mmeval.core.base_metric import BaseMetric\n",
    "from mmeval.core.dispatcher import dispatch\n",
    "from mmeval.utils import try_import\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    import paddle\n",
    "    import tensorflow\n",
    "    import tensorflow as tf\n",
    "    import torch\n",
    "else:\n",
    "    # 使用 try_import \n",
    "    paddle = try_import('paddle')\n",
    "    torch = try_import('torch')\n",
    "    tf = try_import('tensorflow')\n",
    "\n",
    "\n",
    "# 类 docstring 规范，注意添加评测指标使用示例\n",
    "class MeanIoU(BaseMetric):\n",
    "    \"\"\"MeanIoU evaluation metric.\n",
    "\n",
    "    MeanIoU is a widely used evaluation metric for image semantic segmentation.\n",
    "\n",
    "    In addition to mean iou, it will also compute and return accuracy, mean\n",
    "    accuracy, mean dice, mean precision, mean recall and mean f-score.\n",
    "\n",
    "    This metric supports 4 kinds of inputs, i.e. ``numpy.ndarray``,\n",
    "    ``torch.Tensor``, ``tensorflow.Tensor`` and ``paddle.Tensor``, and the\n",
    "    implementation for the calculation depends on the inputs type.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): The number of classes. If None, it will be\n",
    "            obtained from the 'num_classes' or 'classes' field in\n",
    "            `self.dataset_meta`. Defaults to None.\n",
    "        ignore_index (int, optional): Index that will be ignored in evaluation.\n",
    "            Defaults to 255.\n",
    "        nan_to_num (int, optional): If specified, NaN values will be replaced\n",
    "            by the numbers defined by the user. Defaults to None.\n",
    "        beta (int, optional): Determines the weight of recall in the F-score.\n",
    "            Defaults to 1.\n",
    "        classwise_results (bool, optional): Whether to return the computed\n",
    "            results of each class. Defaults to False.\n",
    "        **kwargs: Keyword arguments passed to :class:`BaseMetric`.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> from mmeval import MeanIoU\n",
    "        >>> miou = MeanIoU(num_classes=4)\n",
    "\n",
    "    Use NumPy implementation:\n",
    "\n",
    "        >>> import numpy as np\n",
    "        >>> labels = np.asarray([[[0, 1, 1], [2, 3, 2]]])\n",
    "        >>> preds = np.asarray([[[0, 2, 1], [1, 3, 2]]])\n",
    "        >>> miou(preds, labels)\n",
    "        {'aAcc': 0.6666666666666666,\n",
    "         'mIoU': 0.6666666666666666,\n",
    "         'mAcc': 0.75,\n",
    "         'mDice': 0.75,\n",
    "         'mPrecision': 0.75,\n",
    "         'mRecall': 0.75,\n",
    "         'mFscore': 0.75,\n",
    "         'kappa': 0.5384615384615384}\n",
    "\n",
    "    Use PyTorch implementation:\n",
    "\n",
    "        >>> import torch\n",
    "        >>> labels = torch.Tensor([[[0, 1, 1], [2, 3, 2]]])\n",
    "        >>> preds = torch.Tensor([[[0, 2, 1], [1, 3, 2]]])\n",
    "        >>> miou(preds, labels)\n",
    "        {'aAcc': 0.6666666666666666,\n",
    "         'mIoU': 0.6666666666666666,\n",
    "         'mAcc': 0.75,\n",
    "         'mDice': 0.75,\n",
    "         'mPrecision': 0.75,\n",
    "         'mRecall': 0.75,\n",
    "         'mFscore': 0.75,\n",
    "         'kappa': 0.5384615384615384}\n",
    "\n",
    "    Accumulate batch:\n",
    "\n",
    "        >>> for i in range(10):\n",
    "        ...     labels = torch.randint(0, 4, size=(100, 10, 10))\n",
    "        ...     predicts = torch.randint(0, 4, size=(100, 10, 10))\n",
    "        ...     miou.add(predicts, labels)\n",
    "        >>> miou.compute()  # doctest: +SKIP\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes: Optional[int] = None,\n",
    "                 ignore_index: int = 255,\n",
    "                 nan_to_num: Optional[int] = None,\n",
    "                 beta: int = 1,\n",
    "                 classwise_results: bool = False,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self._num_classes = num_classes\n",
    "        self.ignore_index = ignore_index\n",
    "        self.nan_to_num = nan_to_num\n",
    "        self.beta = beta\n",
    "        self.classwise_results = classwise_results\n",
    "\n",
    "    # num_classes 有两种设置方式，初始化时候设置或者通过 dataset_meta 设置\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        \"\"\"Returns the number of classes.\n",
    "\n",
    "        The number of classes should be set during initialization, otherwise it\n",
    "        will be obtained from the 'classes' or 'num_classes' field in\n",
    "        ``self.dataset_meta``.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If the num_classes is not set.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of classes.\n",
    "        \"\"\"\n",
    "        if self._num_classes is not None:\n",
    "            return self._num_classes\n",
    "        if self.dataset_meta and 'num_classes' in self.dataset_meta:\n",
    "            self._num_classes = self.dataset_meta['num_classes']\n",
    "        elif self.dataset_meta and 'classes' in self.dataset_meta:\n",
    "            self._num_classes = len(self.dataset_meta['classes'])\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                'The `num_claases` is required, and not found in '\n",
    "                f'dataset_meta: {self.dataset_meta}')\n",
    "        return self._num_classes\n",
    "    \n",
    "    # 必须重写 add 方法，在 add 方法里面将中间变量保存到 self._results \n",
    "    # add 方法接受的参数应该尽量简单清晰\n",
    "    def add(self, predictions: Sequence, labels: Sequence) -> None:  # type: ignore # yapf: disable # noqa: E501\n",
    "        \"\"\"Proces one batch of data and predictions.\n",
    "\n",
    "        Calculate the following 3 stuff from the inputs and store them in\n",
    "        ``self._results``:\n",
    "\n",
    "        - num_tp_per_class: the number of true positive per-class.\n",
    "        - num_gts_per_class: the number of ground truth per-class.\n",
    "        - num_preds_per_class: the number of predicition per-class.\n",
    "\n",
    "        Args:\n",
    "            predictions (Sequence): A sequence of the predicted segmentation\n",
    "                mask.\n",
    "            labels (Sequence): A sequence of the segmentation mask labels.\n",
    "        \"\"\"\n",
    "        for prediction, label in zip(predictions, labels):\n",
    "            assert prediction.shape == label.shape, 'The shape of ' \\\n",
    "                '`prediction` and `label` should be the same, but got: ' \\\n",
    "                f'{prediction.shape} and {label.shape}'\n",
    "            # We assert the prediction and label should be a segmentation mask.\n",
    "            assert len(prediction.shape) == 2, 'The dimension of ' \\\n",
    "                f'`prediction` should be 2, but got shape: {prediction.shape}'\n",
    "            # Store the intermediate result used to calculate IoU.\n",
    "            confusion_matrix = self.compute_confusion_matrix(\n",
    "                prediction, label, self.num_classes)\n",
    "            num_tp_per_class = np.diag(confusion_matrix)\n",
    "            num_gts_per_class = confusion_matrix.sum(1)\n",
    "            num_preds_per_class = confusion_matrix.sum(0)\n",
    "            self._results.append(\n",
    "                (num_tp_per_class, num_gts_per_class, num_preds_per_class), )\n",
    "    \n",
    "    # 使用 dispatch 装饰，根据输入的数据类型进行分发，计算混淆矩阵。\n",
    "    @overload  # type: ignore\n",
    "    @dispatch\n",
    "    def compute_confusion_matrix(self, prediction: np.ndarray,\n",
    "                                 label: np.ndarray,\n",
    "                                 num_classes: int) -> np.ndarray:\n",
    "        \"\"\"Compute confusion matrix with NumPy.\n",
    "\n",
    "        Args:\n",
    "            prediction (numpy.ndarray): The predicition.\n",
    "            label (numpy.ndarray): The ground truth.\n",
    "            num_classes (int): The number of classes.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The computed confusion matrix.\n",
    "        \"\"\"\n",
    "        mask = (label != self.ignore_index)\n",
    "        prediction, label = prediction[mask], label[mask]\n",
    "        confusion_matrix_1d = np.bincount(\n",
    "            num_classes * label + prediction, minlength=num_classes**2)\n",
    "        confusion_matrix = confusion_matrix_1d.reshape(num_classes,\n",
    "                                                       num_classes)\n",
    "        return confusion_matrix\n",
    "\n",
    "    @overload  # type: ignore\n",
    "    @dispatch\n",
    "    def compute_confusion_matrix(  # type: ignore\n",
    "            self, prediction: 'torch.Tensor', label: 'torch.Tensor',\n",
    "            num_classes: int) -> np.ndarray:\n",
    "        \"\"\"Compute confusion matrix with PyTorch.\n",
    "\n",
    "        Args:\n",
    "            prediction (torch.Tensor): The predicition.\n",
    "            label (torch.Tensor): The ground truth.\n",
    "            num_classes (int): The number of classes.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The computed confusion matrix.\n",
    "        \"\"\"\n",
    "        mask = (label != self.ignore_index)\n",
    "        prediction, label = prediction[mask], label[mask]\n",
    "        confusion_matrix_1d = torch.bincount(\n",
    "            num_classes * label + prediction, minlength=num_classes**2)\n",
    "        confusion_matrix = confusion_matrix_1d.reshape(num_classes,\n",
    "                                                       num_classes)\n",
    "        return confusion_matrix.cpu().numpy()\n",
    "\n",
    "    @overload\n",
    "    @dispatch\n",
    "    def compute_confusion_matrix(  # type: ignore\n",
    "            self, prediction: 'paddle.Tensor', label: 'paddle.Tensor',\n",
    "            num_classes: int) -> np.ndarray:\n",
    "        \"\"\"Compute confusion matrix with Paddle.\n",
    "\n",
    "        Args:\n",
    "            prediction (paddle.Tensor): The predicition.\n",
    "            label (paddle.Tensor): The ground truth.\n",
    "            num_classes (int): The number of classes.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The computed confusion matrix.\n",
    "        \"\"\"\n",
    "        mask = (label != self.ignore_index)\n",
    "        prediction, label = prediction[mask], label[mask]\n",
    "        # NOTE: Since the `paddle.bincount` has bug on the CUDA device, we use\n",
    "        # the `np.bincount` instead. Once the bug is fixed, we will use\n",
    "        # `paddle.bincount`.\n",
    "        # For more see at: https://github.com/PaddlePaddle/Paddle/issues/46978\n",
    "        confusion_matrix_1d = np.bincount(\n",
    "            num_classes * label + prediction, minlength=num_classes**2)\n",
    "        confusion_matrix = confusion_matrix_1d.reshape(\n",
    "            (num_classes, num_classes))\n",
    "        return confusion_matrix\n",
    "\n",
    "    @dispatch\n",
    "    def compute_confusion_matrix(  # type: ignore\n",
    "            self, prediction: 'tensorflow.Tensor', label: 'tensorflow.Tensor',\n",
    "            num_classes: int) -> np.ndarray:\n",
    "        \"\"\"Compute confusion matrix with TensorFlow.\n",
    "\n",
    "        Args:\n",
    "            prediction (tensorflow.Tensor): The predicition.\n",
    "            label (tensorflow.Tensor): The ground truth.\n",
    "            num_classes (int): The number of classes.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The computed confusion matrix.\n",
    "        \"\"\"\n",
    "        mask = (label != self.ignore_index)\n",
    "        prediction, label = prediction[mask], label[mask]\n",
    "        confusion_matrix_1d = tf.math.bincount(\n",
    "            tf.cast(num_classes * label + prediction, tf.int32),\n",
    "            minlength=num_classes**2)\n",
    "        confusion_matrix = tf.reshape(confusion_matrix_1d,\n",
    "                                      (num_classes, num_classes))\n",
    "        return confusion_matrix.numpy()\n",
    "    \n",
    "    # 必须重写的方法，在 self._results 进程同步之后，进行指标的计算。\n",
    "    # 需要返回字典\n",
    "    def compute_metric(\n",
    "        self,\n",
    "        results: List[Tuple[np.ndarray, np.ndarray, np.ndarray]],\n",
    "    ) -> dict:\n",
    "        \"\"\"Compute the MeanIoU metric.\n",
    "\n",
    "        This method would be invoked in `BaseMetric.compute` after distributed\n",
    "        synchronization.\n",
    "\n",
    "        Args:\n",
    "            results (List[tuple]): This list has already been synced across all\n",
    "                ranks. This is a list of tuple, and each tuple has the\n",
    "                following elements:\n",
    "\n",
    "                - (List[numpy.ndarray]): Each element in the list is the number\n",
    "                  of true positive per-class on a sample.\n",
    "                - (List[numpy.ndarray]): Each element in the list is the number\n",
    "                  of ground truth per-class on a sample.\n",
    "                - (List[numpy.ndarray]): Each element in the list is the number\n",
    "                  of predicition per-class on a sample.\n",
    "\n",
    "        Returns:\n",
    "            Dict: The computed metric, with following keys:\n",
    "\n",
    "            - aAcc, the overall accuracy, namely pixel accuracy.\n",
    "            - mIoU, the mean Intersection-Over-Union (IoU) for all classes.\n",
    "            - mAcc, the mean accuracy for all classes, namely mean pixel\n",
    "            accuracy.\n",
    "            - mDice, the mean dice coefficient for all claases.\n",
    "            - mPrecision, the mean precision for all classes.\n",
    "            - mRecall, the mean recall for all classes.\n",
    "            - mFscore, the mean f-score for all classes.\n",
    "            - kappa, the Cohen's kappa coefficient.\n",
    "            - classwise_result, the evaluate results of each classes.\n",
    "            This would be returned if ``self.classwise_result`` is True.\n",
    "        \"\"\"\n",
    "        # Gather the `num_tp_per_class` from batches results.\n",
    "        num_tp_per_class: np.ndarray = sum(res[0] for res in results)\n",
    "        # Gather the `num_gts_per_class` from batches results.\n",
    "        num_gts_per_class: np.ndarray = sum(res[1] for res in results)\n",
    "        # Gather the `num_preds_per_class` from batches results.\n",
    "        num_preds_per_class: np.ndarray = sum(res[2] for res in results)\n",
    "\n",
    "        # Computing overall accuracy.\n",
    "        overall_acc = num_tp_per_class.sum() / num_gts_per_class.sum()\n",
    "\n",
    "        # compute iou per class\n",
    "        union = num_preds_per_class + num_gts_per_class - num_tp_per_class\n",
    "        iou = num_tp_per_class / union\n",
    "\n",
    "        # compute accuracy per class\n",
    "        accuracy = num_tp_per_class / num_gts_per_class\n",
    "\n",
    "        # compute dice per class\n",
    "        dice = 2 * num_tp_per_class / (num_preds_per_class + num_gts_per_class)\n",
    "\n",
    "        # compute precision, recall and f-score per class\n",
    "        precision = num_tp_per_class / num_preds_per_class\n",
    "        recall = num_tp_per_class / num_gts_per_class\n",
    "        f_score = (1 + self.beta**2) * (precision * recall) / (\n",
    "            (self.beta**2 * precision) + recall)\n",
    "\n",
    "        # compute kappa coefficient\n",
    "        po = num_tp_per_class.sum() / num_gts_per_class.sum()\n",
    "        pe = (num_gts_per_class * num_preds_per_class).sum() / (\n",
    "            num_gts_per_class.sum()**2)\n",
    "        kappa = (po - pe) / (1 - pe)\n",
    "\n",
    "        def _mean(values: np.ndarray):\n",
    "            if self.nan_to_num is not None:\n",
    "                values = np.nan_to_num(values, nan=self.nan_to_num)\n",
    "            return np.nanmean(values)\n",
    "\n",
    "        metric_results = {\n",
    "            'aAcc': overall_acc,\n",
    "            'mIoU': _mean(iou),\n",
    "            'mAcc': _mean(accuracy),\n",
    "            'mDice': _mean(dice),\n",
    "            'mPrecision': _mean(precision),\n",
    "            'mRecall': _mean(recall),\n",
    "            'mFscore': _mean(f_score),\n",
    "            'kappa': kappa,\n",
    "        }\n",
    "\n",
    "        # Add the class-wise metric results to the returned results.\n",
    "        if self.classwise_results:\n",
    "            metric_results['classwise_results'] = {\n",
    "                'IoU': iou,\n",
    "                'Acc': accuracy,\n",
    "                'Dice': dice,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'Fscore': f_score,\n",
    "            }\n",
    "        return metric_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aadeab",
   "metadata": {},
   "source": [
    "## 2.2 在 MMSegmentation 中使用 MeanIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33c09c",
   "metadata": {},
   "source": [
    "见：[open-mmlab/mmsegmentation/pull/2003](https://github.com/open-mmlab/mmsegmentation/pull/2003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e316c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Sequence\n",
    "from mmengine.logging import print_log\n",
    "from prettytable import PrettyTable\n",
    "from mmeval.metrics import MeanIoU\n",
    "\n",
    "from mmseg.registry import METRICS\n",
    "\n",
    "\n",
    "# 继承 mmeval.MeanIoU\n",
    "@METRICS.register_module()\n",
    "class IoUMetric(MeanIoU):\n",
    "    \"\"\"A wrapper of ``mmeval.MeanIoU``.\n",
    "\n",
    "    This wrapper implements the `process` method that parses predictions and \n",
    "    labels from inputs. This enables ``mmengine.Evaluator`` to handle the data\n",
    "    flow of different tasks through a unified interface.\n",
    "\n",
    "    In addition, this wrapper also implements the ``evaluate`` method that\n",
    "    parses metric results and print pretty tabel of metrics per class.\n",
    "\n",
    "    Args:\n",
    "        dist_backend (str | None): The name of the distributed communication\n",
    "            backend. Refer to :class:`mmeval.BaseMetric`.\n",
    "            Defaults to 'torch_cuda'.\n",
    "        **kwargs: Keyword parameters passed to :class:`mmeval.MeanIoU`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dist_backend='torch_cuda', **kwargs):\n",
    "        iou_metrics = kwargs.pop('iou_metrics', None)\n",
    "        if iou_metrics is not None:\n",
    "            warnings.warn(\n",
    "                'DeprecationWarning: The `iou_metrics` parameter of '\n",
    "                '`IoUMetric` is deprecated, defaults return all metrics now!')\n",
    "        collect_device = kwargs.pop('collect_device', None)\n",
    "\n",
    "        if collect_device is not None:\n",
    "            warnings.warn(\n",
    "                'DeprecationWarning: The `collect_device` parameter of '\n",
    "                '`IoUMetric` is deprecated, use `dist_backend` instead.')\n",
    "\n",
    "        # Changes the default value of `classwise_results` to True.\n",
    "        super().__init__(classwise_results=True,\n",
    "                         dist_backend=dist_backend,\n",
    "                         **kwargs)\n",
    "\n",
    "    def process(self, data_batch: dict, data_samples: Sequence[dict]) -> None:\n",
    "        \"\"\"Process one batch of data and data_samples.\n",
    "\n",
    "        Parse predictions and labels from ``data_samples`` and invoke\n",
    "        ``self.add``.\n",
    "\n",
    "        Args:\n",
    "            data_batch (dict): A batch of data from the dataloader.\n",
    "            data_samples (Sequence[dict]): A batch of outputs from the model.\n",
    "        \"\"\"\n",
    "        predictions, labels = [], []\n",
    "        for data_sample in data_samples:\n",
    "            pred_label = data_sample['pred_sem_seg']['data'].squeeze()\n",
    "            label = data_sample['gt_sem_seg']['data'].squeeze().to(pred_label)\n",
    "            predictions.append(pred_label)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.add(predictions, labels)\n",
    "\n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        \"\"\"Returns metric results and print pretty tabel of metrics per class.\n",
    "\n",
    "        This method would be invoked by ``mmengine.Evaluator``.\n",
    "        \"\"\"\n",
    "        metric_results = self.compute(*args, **kwargs)\n",
    "        self.reset()\n",
    "\n",
    "        classwise_results = metric_results['classwise_results']\n",
    "        del metric_results['classwise_results']\n",
    "        \n",
    "        # Pretty table of the metric results per class.\n",
    "        summary_table = PrettyTable()\n",
    "        summary_table.add_column('Class', self.dataset_meta['classes'])\n",
    "        for key, value in classwise_results.items():\n",
    "            value = np.round(value * 100, 2)\n",
    "            summary_table.add_column(key, value)\n",
    "\n",
    "        print_log('per class results:', logger='current')\n",
    "        print_log('\\n' + summary_table.get_string(), logger='current')\n",
    "\n",
    "        # Multiply value by 100 to convert to percentage and rounding. \n",
    "        evaluate_results = {\n",
    "            k: round(v * 100, 2) for k, v in metric_results.items()}\n",
    "        return evaluate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da06b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
